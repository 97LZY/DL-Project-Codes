{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25126,"status":"ok","timestamp":1680591233834,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"8qrwGJpiCp0m","outputId":"79125348-b14f-4fd9-9323-e43031b119af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1680591236478,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"TAiiN8y9C0ex","outputId":"4ab82458-da53-4faa-ede1-4e9063ecd6e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/JHU_Courses/Deep_Learning/Project\n"]}],"source":["cd drive/MyDrive/JHU_Courses/Deep_Learning/Project/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":3267,"status":"ok","timestamp":1680591242264,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"JmeGa-3bC0hH","outputId":"143d847d-ae88-4f32-bcc8-10a57e3b855e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.0.0+cu118'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import torch.nn.init as init\n","from torchvision import models, transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn import DataParallel\n","import os\n","from PIL import Image, ImageOps\n","import time\n","import pickle\n","import numpy as np\n","from torchvision.transforms import Lambda\n","import argparse\n","import copy\n","import random\n","import numbers\n","import multiprocessing as mp\n","torch.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqwxytJrC0jm"},"outputs":[],"source":["# default parameters\n","\n","#gpu_usg = \",\".join(list(map(str, [2])))\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_usg\n","sequence_length = 4\n","train_batch_size = 100\n","val_batch_size = 8\n","optimizer_choice = 1\n","multi_optim = 1\n","epochs = 25\n","workers = 2\n","use_flip = 0\n","crop_type = 1\n","learning_rate = 1e-3\n","momentum = 0.9\n","weight_decay = 0\n","dampening = 0\n","use_nesterov = False\n","sgd_adjust_lr = 1\n","sgd_step = 5\n","sgd_gamma = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1680591284723,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"-ipbN5_xT8eY","outputId":"9c538502-d1f4-49d2-88e8-bb4a103285fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# updated parameters\n","\n","epochs = 15\n","train_batch_size = 300\n","val_batch_size = 80\n","sequence_length = 10\n","#optimizer_choice = 0\n","#use_flip = 1\n","workers=mp.cpu_count()\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","num_gpu = torch.cuda.device_count()\n","use_gpu = torch.cuda.is_available()\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680591286618,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"ian2qyF0C09L","outputId":"376340f9-322b-4ee6-8923-b4263ca1925c"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of gpu   :      1\n","sequence length :     10\n","train batch size:    300\n","valid batch size:     80\n","optimizer choice:      1\n","multiple optim  :      1\n","num of epochs   :     15\n","num of workers  :     12\n","test crop type  :      1\n","whether to flip :      0\n","learning rate   : 0.0010\n","momentum for sgd: 0.9000\n","weight decay    : 0.0000\n","dampening       : 0.0000\n","use nesterov    :      0\n","method for sgd  :      1\n","step for sgd    :      5\n","gamma for sgd   : 0.1000\n"]}],"source":["print('number of gpu   : {:6d}'.format(num_gpu))\n","print('sequence length : {:6d}'.format(sequence_length))\n","print('train batch size: {:6d}'.format(train_batch_size))\n","print('valid batch size: {:6d}'.format(val_batch_size))\n","print('optimizer choice: {:6d}'.format(optimizer_choice))\n","print('multiple optim  : {:6d}'.format(multi_optim))\n","print('num of epochs   : {:6d}'.format(epochs))\n","print('num of workers  : {:6d}'.format(workers))\n","print('test crop type  : {:6d}'.format(crop_type))\n","print('whether to flip : {:6d}'.format(use_flip))\n","print('learning rate   : {:.4f}'.format(learning_rate))\n","print('momentum for sgd: {:.4f}'.format(momentum))\n","print('weight decay    : {:.4f}'.format(weight_decay))\n","print('dampening       : {:.4f}'.format(dampening))\n","print('use nesterov    : {:6d}'.format(use_nesterov))\n","print('method for sgd  : {:6d}'.format(sgd_adjust_lr))\n","print('step for sgd    : {:6d}'.format(sgd_step))\n","print('gamma for sgd   : {:.4f}'.format(sgd_gamma))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAa4njGzC0-U"},"outputs":[],"source":["def pil_loader(path):\n","    with open(path, 'rb') as f:\n","        with Image.open(f) as img:\n","            return img.convert('RGB')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZilsvvgC0_f"},"outputs":[],"source":["class RandomCrop(object):\n","\n","    def __init__(self, size, padding=0):\n","        if isinstance(size, numbers.Number):\n","            self.size = (int(size), int(size))\n","        else:\n","            self.size = size\n","        self.padding = padding\n","        self.count = 0\n","\n","    def __call__(self, img):\n","\n","        if self.padding > 0:\n","            img = ImageOps.expand(img, border=self.padding, fill=0)\n","\n","        w, h = img.size\n","        th, tw = self.size\n","        if w == tw and h == th:\n","            return img\n","\n","        random.seed(self.count // sequence_length)\n","        x1 = random.randint(0, w - tw)\n","        y1 = random.randint(0, h - th)\n","        # print(self.count, x1, y1)\n","        self.count += 1\n","        return img.crop((x1, y1, x1 + tw, y1 + th))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwQb6ijI8h3X"},"outputs":[],"source":["class RandomHorizontalFlip(object):\n","    def __init__(self):\n","        self.count = 0\n","\n","    def __call__(self, img):\n","        seed = self.count // sequence_length\n","        self.count += 1\n","        random.seed(seed)\n","        if random.random() < 0.5:\n","            return img.transpose(Image.FLIP_LEFT_RIGHT)\n","        return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wqx4GcBj8h6N"},"outputs":[],"source":["class CholecDataset(Dataset):\n","    def __init__(self, file_paths, file_labels, transform=None, loader=pil_loader):\n","        self.file_paths = file_paths\n","        self.file_labels_1 = file_labels[:, range(7)]\n","        self.file_labels_2 = file_labels[:, -1]\n","        self.transform = transform\n","        # self.target_transform=target_transform\n","        self.loader = loader\n","\n","    def __getitem__(self, index):\n","        img_names = self.file_paths[index]\n","        labels_1 = self.file_labels_1[index]\n","        labels_2 = self.file_labels_2[index]\n","        imgs = self.loader(img_names)\n","        if self.transform is not None:\n","            imgs = self.transform(imgs)\n","        return imgs, labels_1, labels_2\n","\n","    def __len__(self):\n","        return len(self.file_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aH8r2ZUK8h8P"},"outputs":[],"source":["class multi_lstm(torch.nn.Module):\n","    def __init__(self):\n","        super(multi_lstm, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        #resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","        #resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n","        self.share = torch.nn.Sequential()\n","        self.share.add_module(\"conv1\", resnet.conv1)\n","        self.share.add_module(\"bn1\", resnet.bn1)\n","        self.share.add_module(\"relu\", resnet.relu)\n","        self.share.add_module(\"maxpool\", resnet.maxpool)\n","        self.share.add_module(\"layer1\", resnet.layer1)\n","        self.share.add_module(\"layer2\", resnet.layer2)\n","        self.share.add_module(\"layer3\", resnet.layer3)\n","        self.share.add_module(\"layer4\", resnet.layer4)\n","        self.share.add_module(\"avgpool\", resnet.avgpool)\n","        self.lstm = nn.LSTM(2048, 512, batch_first=True, dropout=1)\n","        self.fc = nn.Linear(512, 7)\n","        self.fc2 = nn.Linear(2048, 7)\n","        #init.xavier_normal(self.lstm.all_weights[0][0])\n","        #init.xavier_normal(self.lstm.all_weights[0][1])\n","        init.xavier_normal_(self.lstm.all_weights[0][0])\n","        init.xavier_normal_(self.lstm.all_weights[0][1])\n","        #init.xavier_uniform(self.fc.weight)\n","        #init.xavier_uniform(self.fc2.weight)\n","        init.xavier_uniform_(self.fc.weight)\n","        init.xavier_uniform_(self.fc2.weight)\n","\n","    def forward(self, x):\n","        x = self.share.forward(x)\n","        x = x.view(-1, 2048)\n","        z = self.fc2(x)\n","        x = x.view(-1, sequence_length, 2048)\n","        self.lstm.flatten_parameters()\n","        y, _ = self.lstm(x)\n","        y = y.contiguous().view(-1, 512)\n","        y = self.fc(y)\n","        return z, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2it72a78iBw"},"outputs":[],"source":["def get_useful_start_idx(sequence_length, list_each_length):\n","    count = 0\n","    idx = []\n","    for i in range(len(list_each_length)):\n","        for j in range(count, count + (list_each_length[i] + 1 - sequence_length)):\n","            idx.append(j)\n","        count += list_each_length[i]\n","    return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzbXDRo39aAW"},"outputs":[],"source":["def get_data(data_path):\n","    with open(data_path, 'rb') as f:\n","        train_test_paths_labels = pickle.load(f)\n","    train_paths = train_test_paths_labels[0]\n","    val_paths = train_test_paths_labels[1]\n","    test_paths = train_test_paths_labels[2]\n","    train_labels = train_test_paths_labels[3]\n","    val_labels = train_test_paths_labels[4]\n","    test_labels = train_test_paths_labels[5]\n","    # the number of corresponding useful sample images for every videos\n","    train_num_each = train_test_paths_labels[6]\n","    val_num_each = train_test_paths_labels[7]\n","    test_num_each = train_test_paths_labels[8]\n","\n","    print('train_paths  : {:6d}'.format(len(train_paths)))\n","    print('train_labels : {:6d}'.format(len(train_labels)))\n","    print('valid_paths  : {:6d}'.format(len(val_paths)))\n","    print('valid_labels : {:6d}'.format(len(val_labels)))\n","    print('test_paths   : {:6d}'.format(len(test_paths)))\n","    print('test_labels  : {:6d}'.format(len(test_labels)))\n","\n","    train_labels = np.asarray(train_labels, dtype=np.int64)\n","    val_labels = np.asarray(val_labels, dtype=np.int64)\n","    test_labels = np.asarray(test_labels, dtype=np.int64)\n","\n","    if use_flip == 0:\n","        train_transforms = transforms.Compose([\n","            RandomCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n","        ])\n","    elif use_flip == 1:\n","        train_transforms = transforms.Compose([\n","            RandomCrop(224),\n","            RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n","        ])\n","\n","    if crop_type == 0:\n","        test_transforms = transforms.Compose([\n","            RandomCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n","        ])\n","    elif crop_type == 1:\n","        test_transforms = transforms.Compose([\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n","        ])\n","    elif crop_type == 5:\n","        test_transforms = transforms.Compose([\n","            transforms.FiveCrop(224),\n","            Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n","            Lambda(\n","                lambda crops: torch.stack(\n","                    [transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])(crop) for crop in crops]))\n","        ])\n","    elif crop_type == 10:\n","        test_transforms = transforms.Compose([\n","            transforms.TenCrop(224),\n","            Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n","            Lambda(\n","                lambda crops: torch.stack(\n","                    [transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])(crop) for crop in crops]))\n","        ])\n","\n","    train_dataset = CholecDataset(train_paths, train_labels, train_transforms)\n","    val_dataset = CholecDataset(val_paths, val_labels, test_transforms)\n","    test_dataset = CholecDataset(test_paths, test_labels, test_transforms)\n","\n","    return train_dataset, train_num_each, val_dataset, val_num_each, test_dataset, test_num_each"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqM3AKBK9aC6"},"outputs":[],"source":["def train_model(train_dataset, train_num_each, val_dataset, val_num_each):\n","    num_train = len(train_dataset)\n","    num_val = len(val_dataset)\n","\n","    train_useful_start_idx = get_useful_start_idx(sequence_length, train_num_each)\n","\n","    val_useful_start_idx = get_useful_start_idx(sequence_length, val_num_each)\n","\n","    num_train_we_use = len(train_useful_start_idx)\n","    num_val_we_use = len(val_useful_start_idx)\n","\n","    train_we_use_start_idx = train_useful_start_idx[0:num_train_we_use]\n","    val_we_use_start_idx = val_useful_start_idx[0:num_val_we_use]\n","\n","    train_idx = []\n","    for i in range(num_train_we_use):\n","        for j in range(sequence_length):\n","            train_idx.append(train_we_use_start_idx[i] + j)\n","\n","    val_idx = []\n","    for i in range(num_val_we_use):\n","        for j in range(sequence_length):\n","            val_idx.append(val_we_use_start_idx[i] + j)\n","\n","    num_train_all = len(train_idx)\n","    num_val_all = len(val_idx)\n","\n","    print('num train start idx : {:6d}'.format(len(train_useful_start_idx)))\n","    print('last idx train start: {:6d}'.format(train_useful_start_idx[-1]))\n","    print('num of train dataset: {:6d}'.format(num_train))\n","    print('num of train we use : {:6d}'.format(num_train_we_use))\n","    print('num of all train use: {:6d}'.format(num_train_all))\n","    print('num valid start idx : {:6d}'.format(len(val_useful_start_idx)))\n","    print('last idx valid start: {:6d}'.format(val_useful_start_idx[-1]))\n","    print('num of valid dataset: {:6d}'.format(num_val))\n","    print('num of valid we use : {:6d}'.format(num_val_we_use))\n","    print('num of all valid use: {:6d}'.format(num_val_all))\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=train_batch_size,\n","        sampler=train_idx,\n","        num_workers=workers,\n","        pin_memory=False\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=val_batch_size,\n","        sampler=val_idx,\n","        num_workers=workers,\n","        pin_memory=False\n","    )\n","    model = multi_lstm()\n","    sig_f = nn.Sigmoid()\n","\n","    sigmoid_cuda = nn.Sigmoid().cuda()\n","\n","    if use_gpu:\n","        model = model.cuda()\n","        sig_f = sig_f.cuda()\n","    model = DataParallel(model)\n","    criterion_1 = nn.BCEWithLogitsLoss(size_average=False)\n","    criterion_2 = nn.CrossEntropyLoss(size_average=False)\n","\n","    if multi_optim == 0:\n","        if optimizer_choice == 0:\n","            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, dampening=dampening,\n","                                  weight_decay=weight_decay, nesterov=use_nesterov)\n","            if sgd_adjust_lr == 0:\n","                exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=sgd_adjust_lr, gamma=sgd_gamma)\n","            elif sgd_adjust_lr == 1:\n","                exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","        elif optimizer_choice == 1:\n","            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    elif multi_optim == 1:\n","        if optimizer_choice == 0:\n","            optimizer = optim.SGD([\n","                {'params': model.module.share.parameters()},\n","                {'params': model.module.lstm.parameters(), 'lr': learning_rate},\n","                {'params': model.module.fc.parameters(), 'lr': learning_rate},\n","                {'params': model.module.fc2.parameters(), 'lr': learning_rate},\n","            ], lr=learning_rate / 10, momentum=momentum, dampening=dampening,\n","                weight_decay=weight_decay, nesterov=use_nesterov)\n","            if sgd_adjust_lr == 0:\n","                exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=sgd_adjust_lr, gamma=sgd_gamma)\n","            elif sgd_adjust_lr == 1:\n","                exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","        elif optimizer_choice == 1:\n","            optimizer = optim.Adam([\n","                {'params': model.module.share.parameters()},\n","                {'params': model.module.lstm.parameters(), 'lr': learning_rate},\n","                {'params': model.module.fc.parameters(), 'lr': learning_rate},\n","                {'params': model.module.fc2.parameters(), 'lr': learning_rate},\n","            ], lr=learning_rate / 10)\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_val_accuracy_1 = 0.0\n","    best_val_accuracy_2 = 0.0  # judge by accu2\n","    correspond_train_acc_1 = 0.0\n","    correspond_train_acc_2 = 0.0\n","\n","    record_np = np.zeros([epochs, 8])\n","\n","    for epoch in range(epochs):\n","        print(\"epoch\"+str(epoch))\n","        # np.random.seed(epoch)\n","        np.random.shuffle(train_we_use_start_idx)\n","        train_idx = []\n","        for i in range(num_train_we_use):\n","            for j in range(sequence_length):\n","                train_idx.append(train_we_use_start_idx[i] + j)\n","\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=train_batch_size,\n","            sampler=train_idx,\n","            num_workers=workers,\n","            pin_memory=False\n","        )\n","\n","        model.train()\n","        train_loss_1 = 0.0\n","        train_loss_2 = 0.0\n","        train_corrects_1 = 0\n","        train_corrects_2 = 0\n","\n","        batch_progress = 0.0\n","\n","        print(\"begin training\")\n","        train_start_time = time.time()\n","        for data in train_loader:\n","            inputs, labels_1, labels_2 = data\n","            if use_gpu:\n","                inputs = Variable(inputs.cuda())\n","                labels_1 = Variable(labels_1.cuda())\n","                labels_2 = Variable(labels_2.cuda())\n","            else:\n","                inputs = Variable(inputs)\n","                labels_1 = Variable(labels_1)\n","                labels_2 = Variable(labels_2)\n","\n","            optimizer.zero_grad()\n","\n","            outputs_1, outputs_2 = model.forward(inputs)\n","\n","            _, preds_2 = torch.max(outputs_2.data, 1)\n","\n","            #sig_out = sig_f(outputs_1.data)\n","            if use_gpu:\n","              sig_out=sigmoid_cuda(outputs_1)\n","            else:\n","              sig_out = sig_f(outputs_1.data)\n","            #preds_1 = torch.ByteTensor(sig_out.cpu() > 0.5)\n","            if use_gpu:\n","              #preds_1=torch.cuda.ByteTensor(sig_out.data > 0.5)\n","              preds_1=torch.cuda.FloatTensor(sig_out.data)\n","            else:\n","              #preds_1 = torch.ByteTensor(sig_out.cpu() > 0.5)\n","              preds_1 = torch.FloatTensor(sig_out.cpu())\n","            \n","            preds_1 = preds_1.long()\n","            \"\"\"\n","            if use_gpu:\n","              preds_1 = (sig_out.data > 0.5).mul_(1)\n","            else:\n","              preds_1 = (sig_out.cpu() > 0.5).mul_(1)\n","            preds_1 = preds_1.float()\n","            \"\"\"\n","            #train_corrects_1 += torch.sum(preds_1 == labels_1.data.cpu())\n","            if use_gpu:\n","              train_corrects_1 += torch.sum(preds_1 == labels_1.data.cuda())\n","            else:\n","              train_corrects_1 += torch.sum(preds_1 == labels_1.data.cpu())\n","            \n","            labels_1 = Variable(labels_1.data.float())\n","            loss_1 = criterion_1(outputs_1, labels_1)\n","\n","            loss_2 = criterion_2(outputs_2, labels_2)\n","            loss = loss_1 + loss_2\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss_1 += loss_1.data.item()\n","            train_loss_2 += loss_2.data.item()\n","            \n","            \n","            train_corrects_2 += torch.sum(preds_2 == labels_2.data)\n","\n","            batch_progress += 1\n","            if batch_progress*train_batch_size >= num_train_all:\n","                percent = 100.0\n","                print('Batch progress: %s [%d/%d]' % (str(percent) + '%', num_train_all, num_train_all), end='\\n')\n","            else:\n","                percent = round(batch_progress*train_batch_size / num_train_all * 100, 2)\n","                print('Batch progress: %s [%d/%d]' % (str(percent) + '%', batch_progress*train_batch_size, num_train_all), end='\\r')\n","\n","        train_elapsed_time = time.time() - train_start_time\n","        print(\"end training\")\n","        train_accuracy_1 = train_corrects_1 / num_train_all / 7\n","        train_accuracy_2 = train_corrects_2 / num_train_all\n","        train_average_loss_1 = train_loss_1 / num_train_all / 7\n","        train_average_loss_2 = train_loss_2 / num_train_all\n","\n","        # begin eval\n","\n","        model.eval()\n","        val_loss_1 = 0.0\n","        val_loss_2 = 0.0\n","        val_corrects_1 = 0\n","        val_corrects_2 = 0\n","        print(\"begin evaluating\")\n","        val_start_time = time.time()\n","        for data in val_loader:\n","            inputs, labels_1, labels_2 = data\n","            labels_2 = labels_2[(sequence_length - 1):: sequence_length]\n","            if use_gpu:\n","                with torch.no_grad():\n","                  inputs = Variable(inputs.cuda())\n","                  labels_1 = Variable(labels_1.cuda())\n","                  labels_2 = Variable(labels_2.cuda())\n","                #inputs = Variable(inputs.cuda(), volatile=True)\n","                #labels_1 = Variable(labels_1.cuda(), volatile=True)\n","                #labels_2 = Variable(labels_2.cuda(), volatile=True)\n","            else:\n","                with torch.no_grad():\n","                  inputs = Variable(inputs)\n","                  labels_1 = Variable(labels_1)\n","                  labels_2 = Variable(labels_2)\n","                #inputs = Variable(inputs, volatile=True)\n","                #labels_1 = Variable(labels_1, volatile=True)\n","                #labels_2 = Variable(labels_2, volatile=True)\n","\n","            if crop_type == 0 or crop_type == 1:\n","                outputs_1, outputs_2 = model.forward(inputs)\n","            elif crop_type == 5:\n","                inputs = inputs.permute(1, 0, 2, 3, 4).contiguous()\n","                inputs = inputs.view(-1, 3, 224, 224)\n","                outputs_1, outputs_2 = model.forward(inputs)\n","                outputs_1 = outputs_1.view(5, -1, 7)\n","                outputs_1 = torch.mean(outputs_1, 0)\n","                outputs_2 = outputs_2.view(5, -1, 7)\n","                outputs_2 = torch.mean(outputs_2, 0)\n","            elif crop_type == 10:\n","                inputs = inputs.permute(1, 0, 2, 3, 4).contiguous()\n","                inputs = inputs.view(-1, 3, 224, 224)\n","                outputs_1, outputs_2 = model.forward(inputs)\n","                outputs_1 = outputs_1.view(10, -1, 7)\n","                outputs_1 = torch.mean(outputs_1, 0)\n","                outputs_2 = outputs_2.view(10, -1, 7)\n","                outputs_2 = torch.mean(outputs_2, 0)\n","\n","            outputs_2 = outputs_2[sequence_length - 1::sequence_length]\n","            _, preds_2 = torch.max(outputs_2.data, 1)\n","\n","            #sig_out = sig_f(outputs_1.data)\n","            if use_gpu:\n","              sig_out=sigmoid_cuda(outputs_1)\n","            else:\n","              sig_out = sig_f(outputs_1.data)\n","\n","            #preds_1 = torch.ByteTensor(sig_out.cpu() > 0.5)\n","            if use_gpu:\n","              #preds_1=torch.cuda.ByteTensor(sig_out.data > 0.5)\n","              preds_1=torch.cuda.FloatTensor(sig_out.data)\n","            else:\n","              #preds_1 = torch.ByteTensor(sig_out.cpu() > 0.5)\n","              preds_1 = torch.FloatTensor(sig_out.cpu())\n","            preds_1 = preds_1.long()\n","\n","            \"\"\"\n","            if use_gpu:\n","              preds_1=(sig_out.data > 0.5).mul_(1)\n","            else:\n","              preds_1 = (sig_out.cpu() > 0.5).mul_(1)\n","            preds_1 = preds_1.float()\n","            \"\"\"\n","\n","            #val_corrects_1 += torch.sum(preds_1 == labels_1.data.cuda())\n","            if use_gpu:\n","              val_corrects_1 += torch.sum(preds_1 == labels_1.data.cuda())\n","            else:\n","              val_corrects_1 += torch.sum(preds_1 == labels_1.data.cpu())\n","\n","\n","\n","            labels_1 = Variable(labels_1.data.float())\n","            loss_1 = criterion_1(outputs_1, labels_1)\n","            \n","          \n","            val_loss_1 += loss_1.data.item()\n","\n","\n","\n","            loss_2 = criterion_2(outputs_2, labels_2)\n","            \n","          \n","            val_loss_2 += loss_2.data.item()\n","            \n","            \n","            val_corrects_2 += torch.sum(preds_2 == labels_2.data)\n","\n","        val_elapsed_time = time.time() - val_start_time\n","        print(\"end evaluating\")\n","        val_accuracy_1 = val_corrects_1 / (num_val_all * 7)\n","        val_accuracy_2 = val_corrects_2 / num_val_we_use\n","        val_average_loss_1 = val_loss_1 / (num_val_all * 7)\n","        val_average_loss_2 = val_loss_2 / num_val_we_use\n","\n","        print('epoch: {:4d}'\n","              ' train time: {:2.0f}m{:2.0f}s'\n","              ' train loss_1: {:4.4f}'\n","              ' train accu_1: {:.4f}'\n","              ' valid time: {:2.0f}m{:2.0f}s'\n","              ' valid loss_1: {:4.4f}'\n","              ' valid accu_1: {:.4f}'\n","              .format(epoch,\n","                      train_elapsed_time // 60,\n","                      train_elapsed_time % 60,\n","                      train_average_loss_1,\n","                      train_accuracy_1,\n","                      val_elapsed_time // 60,\n","                      val_elapsed_time % 60,\n","                      val_average_loss_1,\n","                      val_accuracy_1))\n","        print('epoch: {:4d}'\n","              ' train time: {:2.0f}m{:2.0f}s'\n","              ' train loss_2: {:4.4f}'\n","              ' train accu_2: {:.4f}'\n","              ' valid time: {:2.0f}m{:2.0f}s'\n","              ' valid loss_2: {:4.4f}'\n","              ' valid accu_2: {:.4f}'\n","              .format(epoch,\n","                      train_elapsed_time // 60,\n","                      train_elapsed_time % 60,\n","                      train_average_loss_2,\n","                      train_accuracy_2,\n","                      val_elapsed_time // 60,\n","                      val_elapsed_time % 60,\n","                      val_average_loss_2,\n","                      val_accuracy_2))\n","\n","        if optimizer_choice == 0:\n","            if sgd_adjust_lr == 0:\n","                exp_lr_scheduler.step()\n","            elif sgd_adjust_lr == 1:\n","                exp_lr_scheduler.step(val_average_loss_1 + val_average_loss_2)\n","\n","        #if val_accuracy_2 > best_val_accuracy_2 and val_accuracy_1 > 0.95:\n","        if val_accuracy_2 > best_val_accuracy_2 and val_accuracy_1 > 0.90:\n","            best_val_accuracy_2 = val_accuracy_2\n","            best_val_accuracy_1 = val_accuracy_1\n","            correspond_train_acc_1 = train_accuracy_1\n","            correspond_train_acc_2 = train_accuracy_2\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","        #elif val_accuracy_2 == best_val_accuracy_2 and val_accuracy_1 > 0.95:\n","        elif val_accuracy_2 == best_val_accuracy_2 and val_accuracy_1 > 0.90:\n","            if val_accuracy_1 > best_val_accuracy_1:\n","                correspond_train_acc_1 = train_accuracy_1\n","                correspond_train_acc_2 = train_accuracy_2\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            elif val_accuracy_1 == best_val_accuracy_1:\n","                if train_accuracy_2 > correspond_train_acc_2:\n","                    correspond_train_acc_2 = train_accuracy_2\n","                    correspond_train_acc_1 = train_accuracy_1\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                elif train_accuracy_2 == correspond_train_acc_2:\n","                    if train_accuracy_1 > best_val_accuracy_1:\n","                        correspond_train_acc_1 = train_accuracy_1\n","                        best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        record_np[epoch, 0] = train_accuracy_1\n","        record_np[epoch, 1] = train_accuracy_2\n","        record_np[epoch, 2] = train_average_loss_1\n","        record_np[epoch, 3] = train_average_loss_2\n","        record_np[epoch, 4] = val_accuracy_1\n","        record_np[epoch, 5] = val_accuracy_2\n","        record_np[epoch, 6] = val_average_loss_1\n","        record_np[epoch, 7] = val_average_loss_2\n","\n","        \n","        save_val_1 = int(\"{:4.0f}\".format(best_val_accuracy_1 * 10000))\n","        save_val_2 = int(\"{:4.0f}\".format(best_val_accuracy_2 * 10000))\n","        save_train_1 = int(\"{:4.0f}\".format(correspond_train_acc_1 * 10000))\n","        save_train_2 = int(\"{:4.0f}\".format(correspond_train_acc_2 * 10000))\n","        public_name = \"cnn_lstm\" \\\n","                      + \"_epoch_\" + str(epochs) \\\n","                      + \"_length_\" + str(sequence_length) \\\n","                      + \"_opt_\" + str(optimizer_choice) \\\n","                      + \"_mulopt_\" + str(multi_optim) \\\n","                      + \"_flip_\" + str(use_flip) \\\n","                      + \"_crop_\" + str(crop_type) \\\n","                      + \"_batch_\" + str(train_batch_size) \\\n","                      + \"_train1_\" + str(train_accuracy_1) \\\n","                      + \"_train2_\" + str(train_accuracy_2) \\\n","                      + \"_val1_\" + str(val_accuracy_1) \\\n","                      + \"_val2_\" + str(val_accuracy_2)\n","        model_name = public_name + \".pth\"\n","        torch.save(best_model_wts, model_name)\n","\n","        record_name = public_name + \".npy\"\n","        np.save(record_name, record_np)\n","    print('best accuracy_1: {:.4f} cor train accu_1: {:.4f}'.format(best_val_accuracy_1, correspond_train_acc_1))\n","    print('best accuracy_2: {:.4f} cor train accu_2: {:.4f}'.format(best_val_accuracy_2, correspond_train_acc_2))\n","    save_val_1 = int(\"{:4.0f}\".format(best_val_accuracy_1 * 10000))\n","    save_val_2 = int(\"{:4.0f}\".format(best_val_accuracy_2 * 10000))\n","    save_train_1 = int(\"{:4.0f}\".format(correspond_train_acc_1 * 10000))\n","    save_train_2 = int(\"{:4.0f}\".format(correspond_train_acc_2 * 10000))\n","    public_name = \"cnn_lstm\" \\\n","                  + \"_epoch_\" + str(epochs) \\\n","                  + \"_length_\" + str(sequence_length) \\\n","                  + \"_opt_\" + str(optimizer_choice) \\\n","                  + \"_mulopt_\" + str(multi_optim) \\\n","                  + \"_flip_\" + str(use_flip) \\\n","                  + \"_crop_\" + str(crop_type) \\\n","                  + \"_batch_\" + str(train_batch_size) \\\n","                  + \"_train1_\" + str(save_train_1) \\\n","                  + \"_train2_\" + str(save_train_2) \\\n","                  + \"_val1_\" + str(save_val_1) \\\n","                  + \"_val2_\" + str(save_val_2)\n","    model_name = public_name + \"_best_.pth\"\n","    torch.save(best_model_wts, model_name)\n","\n","    record_name = public_name + \"_best_.npy\"\n","    np.save(record_name, record_np)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831,"status":"ok","timestamp":1680591324003,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"Xf9eDskR9aFp","outputId":"edcd1020-ba6f-4032-96c5-e92015b8de44"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_paths  :  71000\n","train_labels :  71000\n","valid_paths  :  15304\n","valid_labels :  15304\n","test_paths   :  98194\n","test_labels  :  98194\n"]}],"source":["train_dataset, train_num_each, val_dataset, val_num_each, _, _ = get_data('train_val_test_paths_labels.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11585020,"status":"ok","timestamp":1680609751463,"user":{"displayName":"Ziyang Liu","userId":"17797033296740898952"},"user_tz":420},"id":"ACxYCxOf9aIL","outputId":"be7c6546-5d36-41b4-a2a8-3fa6ef1ad5ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["num train start idx :  70712\n","last idx train start:  70990\n","num of train dataset:  71000\n","num of train we use :  70712\n","num of all train use: 707120\n","num valid start idx :  15232\n","last idx valid start:  15294\n","num of valid dataset:  15304\n","num of valid we use :  15232\n","num of all valid use: 152320\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 266MB/s]\n","/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]},{"name":"stdout","output_type":"stream","text":["epoch0\n","begin training\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    0 train time: 43m57s train loss_1: 0.0437 train accu_1: 0.8029 valid time:  9m37s valid loss_1: 0.1047 valid accu_1: 0.7977\n","epoch:    0 train time: 43m57s train loss_2: 0.2095 train accu_2: 0.9284 valid time:  9m37s valid loss_2: 0.8062 valid accu_2: 0.7832\n","epoch1\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    1 train time: 16m13s train loss_1: 0.0133 train accu_1: 0.8461 valid time:  1m52s valid loss_1: 0.1070 valid accu_1: 0.8504\n","epoch:    1 train time: 16m13s train loss_2: 0.0545 train accu_2: 0.9819 valid time:  1m52s valid loss_2: 0.4494 valid accu_2: 0.8719\n","epoch2\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    2 train time: 16m13s train loss_1: 0.0084 train accu_1: 0.8855 valid time:  1m52s valid loss_1: 0.1351 valid accu_1: 0.8691\n","epoch:    2 train time: 16m13s train loss_2: 0.0362 train accu_2: 0.9882 valid time:  1m52s valid loss_2: 0.5730 valid accu_2: 0.8618\n","epoch3\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    3 train time: 16m12s train loss_1: 0.0064 train accu_1: 0.9062 valid time:  1m52s valid loss_1: 0.1248 valid accu_1: 0.8878\n","epoch:    3 train time: 16m12s train loss_2: 0.0260 train accu_2: 0.9917 valid time:  1m52s valid loss_2: 0.5141 valid accu_2: 0.8762\n","epoch4\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    4 train time: 16m12s train loss_1: 0.0047 train accu_1: 0.9229 valid time:  1m52s valid loss_1: 0.1321 valid accu_1: 0.8884\n","epoch:    4 train time: 16m12s train loss_2: 0.0208 train accu_2: 0.9932 valid time:  1m52s valid loss_2: 0.6628 valid accu_2: 0.8388\n","epoch5\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    5 train time: 16m12s train loss_1: 0.0039 train accu_1: 0.9331 valid time:  1m52s valid loss_1: 0.1414 valid accu_1: 0.8994\n","epoch:    5 train time: 16m12s train loss_2: 0.0161 train accu_2: 0.9947 valid time:  1m52s valid loss_2: 0.6133 valid accu_2: 0.8724\n","epoch6\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    6 train time: 16m13s train loss_1: 0.0038 train accu_1: 0.9366 valid time:  1m52s valid loss_1: 0.1343 valid accu_1: 0.9034\n","epoch:    6 train time: 16m13s train loss_2: 0.0186 train accu_2: 0.9940 valid time:  1m52s valid loss_2: 0.6054 valid accu_2: 0.8644\n","epoch7\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    7 train time: 16m13s train loss_1: 0.0031 train accu_1: 0.9392 valid time:  1m52s valid loss_1: 0.1390 valid accu_1: 0.9064\n","epoch:    7 train time: 16m13s train loss_2: 0.0142 train accu_2: 0.9955 valid time:  1m52s valid loss_2: 0.5570 valid accu_2: 0.8706\n","epoch8\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    8 train time: 16m13s train loss_1: 0.0028 train accu_1: 0.9421 valid time:  1m53s valid loss_1: 0.1458 valid accu_1: 0.9174\n","epoch:    8 train time: 16m13s train loss_2: 0.0129 train accu_2: 0.9958 valid time:  1m53s valid loss_2: 0.5561 valid accu_2: 0.8812\n","epoch9\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:    9 train time: 16m13s train loss_1: 0.0025 train accu_1: 0.9467 valid time:  1m52s valid loss_1: 0.1888 valid accu_1: 0.9047\n","epoch:    9 train time: 16m13s train loss_2: 0.0111 train accu_2: 0.9965 valid time:  1m52s valid loss_2: 0.6315 valid accu_2: 0.8694\n","epoch10\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:   10 train time: 16m13s train loss_1: 0.0026 train accu_1: 0.9510 valid time:  1m52s valid loss_1: 0.1414 valid accu_1: 0.9282\n","epoch:   10 train time: 16m13s train loss_2: 0.0112 train accu_2: 0.9964 valid time:  1m52s valid loss_2: 0.6310 valid accu_2: 0.8762\n","epoch11\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:   11 train time: 16m14s train loss_1: 0.0019 train accu_1: 0.9496 valid time:  1m52s valid loss_1: 0.1533 valid accu_1: 0.9121\n","epoch:   11 train time: 16m14s train loss_2: 0.0094 train accu_2: 0.9971 valid time:  1m52s valid loss_2: 0.6272 valid accu_2: 0.8596\n","epoch12\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:   12 train time: 16m14s train loss_1: 0.0021 train accu_1: 0.9529 valid time:  1m53s valid loss_1: 0.1531 valid accu_1: 0.9233\n","epoch:   12 train time: 16m14s train loss_2: 0.0089 train accu_2: 0.9972 valid time:  1m53s valid loss_2: 0.5960 valid accu_2: 0.8761\n","epoch13\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:   13 train time: 16m13s train loss_1: 0.0019 train accu_1: 0.9545 valid time:  1m53s valid loss_1: 0.1707 valid accu_1: 0.9028\n","epoch:   13 train time: 16m13s train loss_2: 0.0096 train accu_2: 0.9970 valid time:  1m53s valid loss_2: 0.5716 valid accu_2: 0.8793\n","epoch14\n","begin training\n","Batch progress: 100.0% [707120/707120]\n","end training\n","begin evaluating\n","end evaluating\n","epoch:   14 train time: 16m14s train loss_1: 0.0018 train accu_1: 0.9573 valid time:  1m52s valid loss_1: 0.1351 valid accu_1: 0.9162\n","epoch:   14 train time: 16m14s train loss_2: 0.0093 train accu_2: 0.9971 valid time:  1m52s valid loss_2: 0.4851 valid accu_2: 0.8942\n","best accuracy_1: 0.9162 cor train accu_1: 0.9573\n","best accuracy_2: 0.8942 cor train accu_2: 0.9971\n"]}],"source":["train_model(train_dataset, train_num_each, val_dataset, val_num_each)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaIiuOukvI-M"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t90qJdQRvJAx"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNxEXThg74XHbUu8v/dSElI","machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
